# File location and type
file_location = "/FileStore/tables/chunk_0.json"
file_type = "json"

# CSV options
infer_schema = "false"
first_row_is_header = "false"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

# display(df)




df = (spark.read
             .option("inferSchema", True)
             .json(file_location)
            )
# ---------------------------------------------------------------------------------------------------------------------

%pip install git+https://github.com/serpapi/google-search-results-python
%pip install requests
%pip install beautifulsoup4
# %pip install matplotlib


# ---------------------------------------------------------------------------------------------------------------------
import pyspark.sql.functions as F
import matplotlib.pyplot as plt
from pyspark.sql.functions import udf, when
from serpapi import GoogleSearch
from pyspark.sql.functions import col, exists, lit
import re
import requests
import difflib
import xml.etree.ElementTree as ET
from pyspark.sql.types import ArrayType, StringType
from pyspark.sql.functions import translate, length
import json
from pyspark.sql.functions import expr
from pyspark.sql.functions import collect_list, size, col
from pyspark.sql.types import LongType
import numpy as np




# #task 1
# You can drop publications with very short titles, e.g., one word, with empty authors

df = df.dropDuplicates()

# Split the title by space and count the number of words
num_words = F.size(F.split(F.col("title"), " "))

# Filter dataset
df = df.filter((F.col("authors").isNotNull()) & (num_words > 1))

# Display result
# display(df)

# ---------------------------------------------------------------------------------------------------------------------


# TASK 3
test1 = df.filter(length(translate(df.issn, '0123456789', '')) == 0) \
            .filter(F.col("issn").isNotNull() & (F.col("issn") !=("")))

# display(test1)

# ---------------------------------------------------------------------------------------------------------------------


#TASK 4 burada nullari sil demir eslinde kodu deyismek olar istesek
test3 = test1.filter(F.col("volume").isNotNull()) \
            .filter((F.col("volume") !=("")))  \
            .filter(F.col("issue").isNotNull())  \
            .filter(F.col("issue") !=("")) \
            .filter(F.col("issue") !=("null"))  \
            .filter(F.col("volume") !=("null"))  \

test4 = test3.withColumn("issue", F.regexp_replace(F.col("issue"), "[^0-9\.]", ""))  \
     .withColumn("issue", F.regexp_extract(F.col("issue"), "([\d\.]+)", 1))  \
     .withColumn("volume", F.regexp_replace(F.col("volume"), "[^0-9\.]", ""))  \
     .withColumn("volume", F.regexp_extract(F.col("volume"), "([\d\.]+)", 1))


display(test4)

# ---------------------------------------------------------------------------------------------------------------------


# TASK 7 START

from pyspark.sql.functions import when
import re
import requests
import difflib
import xml.etree.ElementTree as ET

def search_dblp_api(title):
    url = "https://dblp.org/search/publ/api"
    params = {
        "q": title,
        "format": "xml",
        "expand": "1"
    }

    response = requests.get(url, params=params)

    if response.status_code == 200:
        root = ET.fromstring(response.content)
        venues = []
        for hit in root.iter('hit'):
            title_elem = hit.find('info/title')
            match_ratio = difflib.SequenceMatcher(None, title.lower(), title_elem.text.lower()).ratio()
            if match_ratio >= 0.6:
                venue_elem = hit.find('info/venue')
                if venue_elem is not None:
                    return venue_elem.text
                else:
#                     return None
                    continue
            else:
                continue
        return None
    else:
        return None
    
# print(search_dblp_api('Image Evaluation Factors'))    
  
extract_venue_udf = udf(search_dblp_api)

# df_updated = test2.limit(100).filter(F.col('venue.name_d').isNull()) \
#                   .withColumn('venue_json', extract_venue_udf(F.col('title')))

df_updated = test1.limit(50).withColumn('venue_names',
                                        when(F.col('venue.name_d').isNull(), extract_venue_udf(F.col('title')))  \
                                        .otherwise(F.col('venue.name_d')))
# display(df_updated)


# TASK 7 CONTINUE



import requests
import json

def search_venue_api(venue_name):
    url = "https://dblp.org/search/venue/api"
    params = {
        "q": venue_name ,
        "format": "json",
        "expand": "1"
    }

    response = requests.get(url, params=params)

    if response.status_code == 200:
        data = json.loads(response.content.decode('utf-8'))
        if data.get('result', {}).get('hits', {}).get('@total', '0') == '0':
            return None
        else:
            hit = data['result']['hits'].get('hit', [{}])[0]
            venue_info = hit.get('info', {})
            venue = venue_info.get('venue')
            if venue is not None:
                return venue
            else:
                return None
    else:
        return None
    
extract_search_udf = udf(search_venue_api)

df_venue = df_updated.withColumn('full_venue_names',
                     when(F.col('venue_names').isNotNull(), extract_search_udf(F.col('venue_names')))
                     .otherwise(None))

df_final = df_venue.withColumn('venue', 
            F.struct(*[F.when(F.col('venue.name_d').isNull(), F.col('full_venue_names')).otherwise(F.col('venue.'+c)).alias(c) 
                       if c == 'name_d' else F.col('venue.'+c).alias(c) for c in df_venue.select('venue.*').columns]))

# display(df_final)


# ---------------------------------------------------------------------------------------------------------------------


# TASK 9 GENDER

import requests
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType
import json

from pyspark.sql.functions import expr

new_df = test2.limit(2).withColumn(
    "author_first_names",
    expr("concat_ws(',', transform(authors, x -> split(x.name, ' ')[0]))")
)


# display(new_df)

def get_gender(name):
    api_key = 'rmbEfd93WBfepbVGaVBDDzKQFUVYZVoxkDV7'
    url = f'https://gender-api.com/get?name={name}&key={api_key}'
    response = requests.get(url)
    if response.status_code == 200:
        gender_data = json.loads(response.content.decode('utf-8'))
        gender = gender_data.get('gender')
        return gender
    else:
        return None

get_gender_udf = udf(get_gender)

new_df2 = new_df.withColumn(
    "genders",
    udf(lambda x: [get_gender(name) for name in  x.split(',')], ArrayType(StringType()))("author_first_names")
)
# display(new_df2) 


# ---------------------------------------------------------------------------------------------------------------------



## TASK 10 COMPLETE VERSION++
from pyspark.sql.functions import collect_list, size, col
from pyspark.sql.functions import udf
from pyspark.sql.types import LongType

import numpy as np


df_explode = df_final.limit(50).selectExpr("*", "explode(authors) as author", "author.name as author_name")

df_agg = df_explode.groupby("author_name") \
                  .agg(collect_list("n_citation").alias("n_citations"))

df_agg = df_agg.filter(size((col("n_citations")))!=0) \
               .filter(col("n_citations").isNotNull())


def h_index_expert(citations):
 
    if isinstance(citations, np.ndarray) and citations.size == 1:
        citations = citations.tolist()
 
    citations = list(map(int, citations))
    citations.sort(reverse=True)
    h_idx = 0
    for i, c in enumerate(citations):
        if i < c:
            h_idx += 1
        else:
            break
    return h_idx  
h_index = udf(h_index_expert, LongType())

df_h_index = df_agg.withColumn("h_index", h_index(df_agg.n_citations))

df_final_agg = df_explode.join(df_h_index, ["author_name"], "right")

df_final_agg = df_final_agg.select("*")

display(df_final_agg)